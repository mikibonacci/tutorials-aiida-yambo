{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c65c1d1",
   "metadata": {},
   "source": [
    "# YamboWorkflow to easily compute several quasiparticle corrections\n",
    "\n",
    "Often, several quasiparticle corrections (>100) needs to be computed: we may want to computed interpolated G0W0 bands (with `yambopy` or `wannier90`, for example), or we need it to solve the Bethe-Salpeter equation on top of G0W0 results, which represent the state-of-the-art protocol to compute optical properties of materials.\n",
    "\n",
    "However, this task is a really time consuming: we need to split into several simulation, and the merge back the `ndb.QP` databases containing the quasiparticle corrections. \n",
    "Routinely, this can be done by hands, for a small number of quasiparticle needed, within the `yambopy` package.\n",
    "\n",
    "When we are talking of large set of simulations, we can exploit the power of AiiDA to automatically obtain the final quasiparticle database.  \n",
    "\n",
    "The logic is simple: divide et impera. The workflow decides how to distribute the quasiparticle corrections among the calculations, following input parameters provided by the user. \n",
    "This represents the main difference with the standard `YamboWorklow` run. \n",
    "Then, under the hood, the plugin call `yambopy` to perform the final merging.\n",
    "\n",
    "This tutorial will proceed as the previous one, expect for the fact that, before the submission, we will provide the information needed to compute the wanted quasiparticles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbb14967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida import orm, load_profile\n",
    "load_profile()\n",
    "\n",
    "from aiida.plugins import WorkflowFactory\n",
    "YamboWorkflow = WorkflowFactory('yambo.yambo.yambowf')\n",
    "\n",
    "from aiida_quantumespresso.common.types import ElectronicType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320dda35",
   "metadata": {},
   "source": [
    "## Providing the minimal inputs needed for protocols\n",
    "\n",
    "We have to provide minimal inputs for the creation of the builder instance, namely:\n",
    "- codes;\n",
    "- structure;\n",
    "\n",
    "Providing a parent calculation as input, the already performed steps are skipped, in order to avoid waste of human and computational time.\n",
    "If no parent is passed to the builder, also DFT inputs are created within the protocols as provided in the `PwBaseWorkChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02086337",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'pwcode_id': 'qe.pw@local_slurm', \n",
    "    'pseudo_family':\"PseudoDojo/0.4/PBE/SR/standard/upf\",\n",
    "    'yamboprecode_id':'p2y-5.2.1@local_slurm',\n",
    "    'yambocode_id':'yambo-5.2.1@local_slurm',\n",
    "    'protocol':'fast',\n",
    "    #'parent_id':274, #not necessary to set; if you want it, take ytheour previously nscf id (pk) to skip the DFT part.\n",
    "    'structure_id':195,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44218db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the main inputs:\n",
      "BndsRnXp = 200\n",
      "GbndRnge = 200\n",
      "NGsBlkXp = 6 Ry\n",
      "FFTGvecs = 18 Ry\n",
      "\n",
      "\n",
      "kpoint mesh for nscf: [6, 6, 2]\n"
     ]
    }
   ],
   "source": [
    "builder = YamboWorkflow.get_builder_from_protocol(\n",
    "            pw_code = options['pwcode_id'],\n",
    "            preprocessing_code = options['yamboprecode_id'],\n",
    "            code = options['yambocode_id'],\n",
    "            protocol=options['protocol'],\n",
    "            protocol_qe=options['protocol'],\n",
    "            structure= orm.load_node(options['structure_id']),\n",
    "            overrides={},\n",
    "            pseudo_family= options['pseudo_family'],\n",
    "            #parent_folder=orm.load_node(options['parent_id']).outputs.remote_folder,\n",
    "            electronic_type=ElectronicType.INSULATOR, #default is METAL: in that case, smearing is used\n",
    "            calc_type='gw', #or 'bse'; default is 'gw'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16824aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fast': {'description': 'Under converged for most materials, but fast'},\n",
       " 'moderate': {'description': 'Meta converged for most materials, higher computational cost than fast'},\n",
       " 'precise': {'description': 'Converged for most materials, higher computational cost than moderate'}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can also try different protocols:\n",
    "    \n",
    "YamboWorkflow.get_available_protocols()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d139796",
   "metadata": {},
   "source": [
    "Now, if you inspect the prepopulated inputs, you can see the default values respecting the imposed protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "262cd304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CONTROL': {'calculation': 'nscf',\n",
       "  'forc_conv_thr': 0.001,\n",
       "  'tprnfor': True,\n",
       "  'tstress': True,\n",
       "  'etot_conv_thr': 0.0004},\n",
       " 'SYSTEM': {'nosym': False,\n",
       "  'occupations': 'fixed',\n",
       "  'ecutwfc': 60.0,\n",
       "  'ecutrho': 480.0,\n",
       "  'force_symmorphic': True,\n",
       "  'nbnd': 200},\n",
       " 'ELECTRONS': {'electron_maxstep': 80,\n",
       "  'mixing_beta': 0.4,\n",
       "  'conv_thr': 1.6e-09}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.nscf.pw.parameters.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3929287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arguments': ['dipoles', 'ppa', 'HF_and_locXC', 'gw0'],\n",
       " 'variables': {'Chimod': 'hartree',\n",
       "  'DysSolver': 'n',\n",
       "  'GTermKind': 'BG',\n",
       "  'X_and_IO_nCPU_LinAlg_INV': [1, ''],\n",
       "  'NGsBlkXp': [6, 'Ry'],\n",
       "  'FFTGvecs': [18, 'Ry'],\n",
       "  'BndsRnXp': [[1, 200], ''],\n",
       "  'GbndRnge': [[1, 200], ''],\n",
       "  'QPkrange': [[[1, 1, 32, 32]], '']}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.yres.yambo.parameters.get_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309d56b",
   "metadata": {},
   "source": [
    "We then provide the computational resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "420d057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.scf.pw.metadata.options = {\n",
    "    'max_wallclock_seconds': 1*60*60, # in seconds\n",
    "    'resources': {\n",
    "            \"num_machines\": 1, # nodes\n",
    "            \"num_mpiprocs_per_machine\": 1, # MPI per nodes\n",
    "            \"num_cores_per_mpiproc\": 1, # OPENMP\n",
    "        },\n",
    "    'prepend_text': u\"export OMP_NUM_THREADS=\"+str(1), # if needed\n",
    "    #'account':'project_name',\n",
    "    #'queue_name':'s3par',\n",
    "    #'qos':'',\n",
    "}\n",
    "\n",
    "builder.nscf.pw.metadata.options = builder.scf.pw.metadata.options\n",
    "builder.yres.yambo.metadata.options = builder.scf.pw.metadata.options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192f5e0",
   "metadata": {},
   "source": [
    "### Overrides\n",
    "\n",
    "As in the previous examples (see e.g. then  `YamboRestart` notebook), it is possible to modify the default inputs also during the builder creation phase, so not a posteriori. This can be done by using overrides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4ed1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides_scf = {\n",
    "        'pseudo_family': \"PseudoDojo/0.4/PBE/SR/standard/upf\", \n",
    "        'pw':{\n",
    "            \n",
    "        'metadata':{\n",
    "                    'options':{\n",
    "                    'max_wallclock_seconds': 1*60*60, # in seconds\n",
    "                    'resources': {\n",
    "                            \"num_machines\": 1, # nodes\n",
    "                            \"num_mpiprocs_per_machine\": 1, # MPI per nodes\n",
    "                            \"num_cores_per_mpiproc\": 1, # OPENMP\n",
    "                        },\n",
    "                    'prepend_text': u\"export OMP_NUM_THREADS=\"+str(1), # if needed\n",
    "                    #'account':'project_name',\n",
    "                    #'queue_name':'s3par',\n",
    "                    #'qos':'',\n",
    "                                    },\n",
    "        },\n",
    "        },\n",
    "    }\n",
    "\n",
    "overrides_nscf = {\n",
    "        'pseudo_family': \"PseudoDojo/0.4/PBE/SR/standard/upf\", \n",
    "        'pw': {\n",
    "            'parameters':{\n",
    "                'CONTROL':{}, #not needed if you don't override something\n",
    "                'SYSTEM':{},\n",
    "                'ELECTRONS':{'diagonalization':'cg'},\n",
    "            },\n",
    "             'metadata':{\n",
    "                    'options':{\n",
    "                    'max_wallclock_seconds': 60*60, # in seconds\n",
    "                    'resources': {\n",
    "                            \"num_machines\": 1, # nodes\n",
    "                            \"num_mpiprocs_per_machine\": 1, # MPI per nodes\n",
    "                            \"num_cores_per_mpiproc\": 1, # OPENMP\n",
    "                        },\n",
    "                    'prepend_text': u\"export OMP_NUM_THREADS=\"+str(1), # if needed\n",
    "                    #'account':'project_name',\n",
    "                    #'queue_name':'s3par',\n",
    "                    #'qos':'',\n",
    "                                    },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "overrides_yambo = {\n",
    "        \"yambo\": {\n",
    "            \"parameters\": {\n",
    "                \"arguments\": [\n",
    "                    \"rim_cut\",\n",
    "                ],\n",
    "                \"variables\": {\n",
    "                    \"NGsBlkXp\": [4, \"Ry\"],\n",
    "                    \"FFTGvecs\": [24, \"Ry\"],\n",
    "                },\n",
    "            },\n",
    "        'metadata':{\n",
    "                    'options':{\n",
    "                    'max_wallclock_seconds': 60*60, # in seconds\n",
    "                    'resources': {\n",
    "                            \"num_machines\": 1, # nodes\n",
    "                            \"num_mpiprocs_per_machine\": 1, # MPI per nodes\n",
    "                            \"num_cores_per_mpiproc\": 1, # OPENMP\n",
    "                        },\n",
    "                    'prepend_text': u\"export OMP_NUM_THREADS=\"+str(1), # if needed, i.e. in PBS/Torque \n",
    "                    #'account':'project_name',\n",
    "                    #'queue_name':'s3par',\n",
    "                    #'qos':'',\n",
    "                                    },\n",
    "                    },\n",
    "        },\n",
    "    \n",
    "}\n",
    "\n",
    "overrides = {\n",
    "    'yres': overrides_yambo,\n",
    "    'nscf': overrides_nscf,\n",
    "    'scf': overrides_scf\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db71a1",
   "metadata": {},
   "source": [
    "So, let's create a new builder instance with also the `overrides` information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37477119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the main inputs:\n",
      "BndsRnXp = 200\n",
      "GbndRnge = 200\n",
      "NGsBlkXp = 4 Ry\n",
      "FFTGvecs = 24 Ry\n",
      "\n",
      "\n",
      "kpoint mesh for nscf: [6, 6, 2]\n"
     ]
    }
   ],
   "source": [
    "builder = YamboWorkflow.get_builder_from_protocol(\n",
    "            pw_code = options['pwcode_id'],\n",
    "            preprocessing_code = options['yamboprecode_id'],\n",
    "            code = options['yambocode_id'],\n",
    "            protocol=options['protocol'],\n",
    "            protocol_qe=options['protocol'],\n",
    "            structure= orm.load_node(options['structure_id']),\n",
    "            overrides=overrides,\n",
    "            #parent_folder=load_node(options['parent_id']).outputs.remote_folder,\n",
    "            electronic_type=ElectronicType.INSULATOR, #default is METAL: smearing is used\n",
    "            calc_type='gw', #or 'bse'; default is 'gw'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a34d16c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CONTROL': {'calculation': 'nscf',\n",
       "  'forc_conv_thr': 0.001,\n",
       "  'tprnfor': True,\n",
       "  'tstress': True,\n",
       "  'etot_conv_thr': 0.0004},\n",
       " 'SYSTEM': {'nosym': False,\n",
       "  'occupations': 'fixed',\n",
       "  'ecutwfc': 84.0,\n",
       "  'ecutrho': 336.0,\n",
       "  'force_symmorphic': True,\n",
       "  'nbnd': 200},\n",
       " 'ELECTRONS': {'electron_maxstep': 80,\n",
       "  'mixing_beta': 0.4,\n",
       "  'diagonalization': 'cg',\n",
       "  'conv_thr': 1.6e-09}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.nscf.pw.parameters.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "565e0e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stash': {}, 'resources': {'num_machines': 1, 'num_mpiprocs_per_machine': 1, 'num_cores_per_mpiproc': 1}, 'max_wallclock_seconds': 3600, 'withmpi': True, 'prepend_text': 'export OMP_NUM_THREADS=1'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.yres.yambo.metadata.options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5201d49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cg'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.nscf.pw.parameters.get_dict()['ELECTRONS']['diagonalization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3582720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "family = orm.load_group(\"PseudoDojo/0.4/PBE/SR/standard/upf\")\n",
    "#builder.<sublevels_up_to .pw>.pseudos = family.get_pseudos(structure=structure) \n",
    "builder.scf.pw.pseudos = family.get_pseudos(structure=orm.load_node(195)) \n",
    "builder.nscf.pw.pseudos = family.get_pseudos(structure=orm.load_node(195)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93d1cc",
   "metadata": {},
   "source": [
    "### Requesting the YamboWorkflow to compute a specific quantity: the minimum band gap and the direct band gap at Gamma\n",
    "\n",
    "Within `YamboWorkflow`, it is possible to obtain the band gap of a material in an automatic fashion. The workflow contains the logic to inspect DFT band structure, as computed in the nscf step,\n",
    "and determine the k-points and electronic band coordinates corresponding to the minimal band gap of the material.\n",
    "In this way, the exact quasiparticle levels can be computed, without additional human intervention. \n",
    "\n",
    "Here below we see how to set additional parsing, through the `additional_parsing` attribute of the builder. This consists in an AiiDA List instance containing strings, each of them\n",
    "representing the desired quantity. In this case, we want to compute the band gap at Gamma and the minimal gap, respectively \"gap_GG\" and \"gap_\".\n",
    "\n",
    "It is possible also to ask for other high-symmetry points, e.g. M, K. However, if the points are not contained in our mesh, their quasiparticle correction is skipped (it cannot be computed). \n",
    "Indirect gaps can be computed, providing a string of the type \"gap_AB\", where `A` is the k-point for the top valence band, and `B` is the k-points of the bottom conduction bands. For example, the indirect gap G->M \n",
    "can be computed providing the \"gap_GM\" string in the `additional_parsing` List.\n",
    "\n",
    "Finally, also single particle levels can be computed for the last valence and first conduction bands. What we need to provide is the string \"homo_K\" or \"lumo_K\", respectively. `K` is the desired high-symmetry k-point.\n",
    "To explicitly compute the top valence and the bottom conduction GW energies, just provide \"homo\" and \"lumo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8847782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.additional_parsing = orm.List(list=['gap_GG','gap_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96b057",
   "metadata": {},
   "source": [
    "### Requesting the YamboWorkflow to compute a specific set of quasiparticles\n",
    "\n",
    "The idea is to split the QP calculation in several subsets, then merge it in a final database -- with yambopy functionalities.\n",
    "There are a lot of possibilities to run QP calculations, to be provided in the QP_subset_dict input of the YamboWorkflow: \n",
    "\n",
    "(1) provide subset of already wanted QP, already in subsets (i.e. already splitted);\n",
    "\n",
    "```python\n",
    "QP_subset_dict= {\n",
    "    'subsets':[\n",
    "        [[1,1,8,9],[2,2,8,9]], #first subset\n",
    "        [[3,3,8,9],[4,4,8,9]], #second subset\n",
    "                ],\n",
    "}\n",
    "```\n",
    "\n",
    "(2) provide explicit QP, i.e. a list of single QP to be splitted;\n",
    "\n",
    "```python\n",
    "QP_subset_dict= {\n",
    "    'explicit':[\n",
    "        [1,1,8,9],[2,2,8,9],[3,3,8,9],[4,4,8,9], #to be splitted\n",
    "                ],\n",
    "}\n",
    "```\n",
    "\n",
    "(3) provide boundaries for the bands to be computed: [k_i,k_f,b_i,b_f];\n",
    "\n",
    "```python\n",
    "QP_subset_dict= {\n",
    "    'boundaries':{\n",
    "        'k_i':1,    #default=1\n",
    "        'k_f':20,   #default=NK_ibz\n",
    "        'b_i':8,\n",
    "        'b_f':9,\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "(4) provide a range of (DFT) energies where to consider the bands and the k-points to be computed, useful if we don't know the system;\n",
    "    of we want BSE for given energies -- usually, BSE spectra is well converged for 75% of this range. These are generated as \n",
    "    explicit QP, then splitted.\n",
    "    It is possible to provide also: 'range_spectrum', which find the bands to be included in the BSE calculation, including the other bands \n",
    "    outside the range_QP window as scissored -- automatically by yambo in the BSE calc. So the final QP will have \n",
    "    rangeQP bands, but the BSE calc will have all the range_spectrum bands.\n",
    "    These ranges are windows of 2*range, centered at the Fermi level. \n",
    "    If you set the key 'full_bands'=True, all the kpoints are included for each bands. otherwise, only the qp in the window.\n",
    "\n",
    "```python\n",
    "QP_subset_dict= {\n",
    "    'range_QP':3, #eV         , default=nscf_gap_eV*1.2\n",
    "    'range_spectrum':10, #eV\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "for (2) and (4) there are additional options:\n",
    "    - (a) 'split_bands': split also in bands, not only kpoints the subset. default is True.\n",
    "    - (b) 'extend_QP': it allows to extend the qp after the merging, including QP not explicitely computed\n",
    "        as FD+scissored corrections (see paper HT M Bonacci et al. 2023). Useful in G0W0 interpolations\n",
    "        e.g. within the aiida-yambo-wannier90 plugin.\n",
    "        (b.1) 'consider_only': bands to be only considered explcitely, so the other ones are deleted from the explicit subsets;\n",
    "        (b.2) 'T_smearing': the fake smearing temperature of the correction.\n",
    "\n",
    "```python\n",
    "QP_subset_dict.update({\n",
    "    'split_bands':True, #default\n",
    "    'extend_QP': True, #default is False\n",
    "    'consider_only':[8,9],\n",
    "    'T_smearing':1e-2, #default\n",
    "})\n",
    "```\n",
    "\n",
    "computation options: \n",
    "\n",
    "(a) 'qp_per_subset':20; #how many qp in each splitted subset.\n",
    "(b) 'parallel_runs':4; to be submitted at the same time remotely. then the remote is deleted, as the qp is stored locally,\n",
    "(c) 'resources':para_QP, #see below\n",
    "(d) 'parallelism':res_QP, #see below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf66e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_QP = {}\n",
    "para_QP['SE_CPU'] = '2 2 4'\n",
    "para_QP['SE_ROLEs'] = 'q qp b'\n",
    "res_QP = {\n",
    "                        'num_machines': 1,\n",
    "                        'num_mpiprocs_per_machine': 1,\n",
    "                        'num_cores_per_mpiproc': 1,\n",
    "            }\n",
    "\n",
    "\n",
    "QP_subset_dict= {\n",
    "    'range_QP':10, #eV         , default=nscf_gap_eV*1.2\n",
    "    'full_bands':True,\n",
    "    'consider_only':[8,9], #eV\n",
    "    'qp_per_subset': 10,\n",
    "    'parallel_runs':4,\n",
    "\n",
    "}\n",
    "\n",
    "QP_subset_dict.update({\n",
    "    'resources':res_QP, #default is the same as previous GW\n",
    "    'parallelism': para_QP, #default is the same as previous GW\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "builder.QP_subset_dict= orm.Dict(dict=QP_subset_dict) #set this if you want to compute also QP after the single GW calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e482c",
   "metadata": {},
   "source": [
    "### Submission phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb337ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiida.engine import submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d34e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ab19daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/.conda/envs/aiida/lib/python3.9/site-packages/aiida_quantumespresso/calculations/pw.py:211: UserWarning: `parent_folder` not provided for `nscf` calculation. For work chains wrapping this calculation, you can disable this warning by excluding the `parent_folder` when exposing the inputs of the `PwCalculation`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uuid: 089ee3ab-5ece-48bc-bd95-542148f53638 (pk: 682) (aiida.workflows:yambo.yambo.yambowf)\n"
     ]
    }
   ],
   "source": [
    "if run:\n",
    "    print('run is already running -> {}'.format(run.pk))\n",
    "    print('sure that you want to run again?, if so, copy the else instruction in the cell below and run!')\n",
    "else:\n",
    "    run = submit(builder)\n",
    "\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904aae0d",
   "metadata": {},
   "source": [
    "### Inspecting the outputs\n",
    "\n",
    "Suppose that your calculation finished successfully, then you can access the outputs via the output method of the run instance. All the outputs of YamboRestart and YamboCalculation are inherited here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8202754e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.is_finished_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9732396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[22m2024-01-15 18:48:50 [118 | REPORT]: [682|YamboWorkflow|start_workflow]: no previous pw calculation found, we will start from scratch\n",
      "2024-01-15 18:48:50 [119 | REPORT]: [682|YamboWorkflow|start_workflow]:  workflow initilization step completed.\n",
      "2024-01-15 18:48:51 [120 | REPORT]: [682|YamboWorkflow|can_continue]: the workflow continues with a scf calculation\n",
      "2024-01-15 18:48:51 [121 | REPORT]: [682|YamboWorkflow|perform_next]: performing a scf calculation\n",
      "2024-01-15 18:48:58 [122 | REPORT]:   [687|PwBaseWorkChain|run_process]: launching PwCalculation<692> iteration #1\n",
      "2024-01-15 18:49:22 [123 | REPORT]:   [687|PwBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2024-01-15 18:49:23 [124 | REPORT]:   [687|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2024-01-15 18:49:23 [125 | REPORT]: [682|YamboWorkflow|can_continue]: the workflow continues with a nscf calculation\n",
      "2024-01-15 18:49:23 [126 | REPORT]: [682|YamboWorkflow|perform_next]: performing a nscf calculation\n",
      "2024-01-15 18:49:24 [127 | REPORT]:   [698|PwBaseWorkChain|run_process]: launching PwCalculation<701> iteration #1\n",
      "2024-01-15 18:56:40 [142 | REPORT]:   [698|PwBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2024-01-15 18:56:40 [143 | REPORT]:   [698|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2024-01-15 18:56:40 [144 | REPORT]: [682|YamboWorkflow|can_continue]: the workflow continues with a yambo calculation\n",
      "2024-01-15 18:56:40 [145 | REPORT]: [682|YamboWorkflow|perform_next]: performing a yambo calculation\n",
      "2024-01-15 18:56:40 [146 | REPORT]: [682|YamboWorkflow|perform_next]: updating yambo parameters to parse more results\n",
      "2024-01-15 18:56:41 [147 | REPORT]: [682|YamboWorkflow|perform_next]: {'dft_predicted': 'semiconductor/insulator', 'valence': 8, 'conduction': 9, 'number_of_kpoints': 14, 'nscf_gap_eV': 4.286, 'homo_k': 14, 'lumo_k': 7, 'gap_type': 'indirect', 'gap_': [[14, 14, 8, 8], [7, 7, 9, 9]], 'soc': False, 'magnetic_calculation': False, 'gap_GG': [[1, 1, 8, 8], [1, 1, 9, 9]]}\n",
      "2024-01-15 18:56:42 [148 | REPORT]:   [722|YamboRestart|run_process]: launching YamboCalculation<723> iteration #1\n",
      "2024-01-15 18:57:21 [149 | REPORT]:   [722|YamboRestart|results]: work chain completed after 1 iterations\n",
      "2024-01-15 18:57:21 [150 | REPORT]:   [722|YamboRestart|on_terminated]: remote folders will not be cleaned\n",
      "2024-01-15 18:57:21 [151 | REPORT]: [682|YamboWorkflow|can_continue]: the workflow continues with a QP splitter calculation\n",
      "2024-01-15 18:57:21 [152 | REPORT]: [682|YamboWorkflow|perform_next]: performing a QP splitter calculation\n",
      "2024-01-15 18:57:21 [153 | REPORT]: [682|YamboWorkflow|perform_next]: range of energy for QP: 10 eV\n",
      "2024-01-15 18:57:23 [154 | REPORT]: [682|YamboWorkflow|perform_next]: subsets: [[[1, 1, 8, 8], [1, 1, 9, 9], [2, 2, 8, 8], [2, 2, 9, 9], [3, 3, 8, 8], [3, 3, 9, 9], [4, 4, 8, 8], [4, 4, 9, 9], [5, 5, 8, 8], [5, 5, 9, 9]], [[6, 6, 8, 8], [6, 6, 9, 9], [7, 7, 8, 8], [7, 7, 9, 9], [8, 8, 8, 8], [8, 8, 9, 9], [9, 9, 8, 8], [9, 9, 9, 9], [10, 10, 8, 8], [10, 10, 9, 9]], [[11, 11, 8, 8], [11, 11, 9, 9], [12, 12, 8, 8], [12, 12, 9, 9], [13, 13, 8, 8], [13, 13, 9, 9], [14, 14, 8, 8], [14, 14, 9, 9]]]\n",
      "2024-01-15 18:57:24 [155 | REPORT]: [682|YamboWorkflow|perform_next]: launchiing YamboRestart <732> for QP, iteration#1\n",
      "2024-01-15 18:57:24 [156 | REPORT]: [682|YamboWorkflow|perform_next]: launchiing YamboRestart <734> for QP, iteration#2\n",
      "2024-01-15 18:57:25 [157 | REPORT]: [682|YamboWorkflow|perform_next]: launchiing YamboRestart <736> for QP, iteration#3\n",
      "2024-01-15 18:57:25 [158 | REPORT]:   [732|YamboRestart|run_process]: launching YamboCalculation<737> iteration #1\n",
      "2024-01-15 18:57:26 [159 | REPORT]:   [734|YamboRestart|run_process]: launching YamboCalculation<738> iteration #1\n",
      "2024-01-15 18:57:27 [160 | REPORT]:   [736|YamboRestart|run_process]: launching YamboCalculation<739> iteration #1\n",
      "2024-01-15 18:57:56 [161 | REPORT]:   [732|YamboRestart|results]: work chain completed after 1 iterations\n",
      "2024-01-15 18:57:56 [162 | REPORT]:   [732|YamboRestart|on_terminated]: cleaned remote folders of calculations: 737\n",
      "2024-01-15 18:58:01 [163 | REPORT]:   [734|YamboRestart|results]: work chain completed after 1 iterations\n",
      "2024-01-15 18:58:01 [164 | REPORT]:   [734|YamboRestart|on_terminated]: cleaned remote folders of calculations: 738\n",
      "2024-01-15 18:58:13 [165 | REPORT]:   [736|YamboRestart|results]: work chain completed after 1 iterations\n",
      "2024-01-15 18:58:13 [166 | REPORT]:   [736|YamboRestart|on_terminated]: cleaned remote folders of calculations: 739\n",
      "2024-01-15 18:58:13 [167 | REPORT]: [682|YamboWorkflow|can_continue]: workflow is finished\n",
      "2024-01-15 18:58:14 [168 | REPORT]: [682|YamboWorkflow|post_processing_needed]: merge QP needed\n",
      "2024-01-15 18:58:14 [169 | REPORT]: [682|YamboWorkflow|run_post_process]: run merge QP\n",
      "2024-01-15 18:58:40 [170 | REPORT]: [682|YamboWorkflow|report_wf]: PARSED: {'gap_GG': 7.337081552088261, 'homo_G': -1.6128203392267229, 'lumo_G': 5.724261212861538, 'gap_GG_dft': 6.783010197755695, 'homo_G_dft': -1.281881376257539, 'lumo_G_dft': 5.501128821498156, 'gap_': 5.354011228048802, 'homo': -0.22572098498940468, 'lumo': 5.128290243059397, 'gap_dft': 4.2863655744910245, 'homo_dft': 0.0, 'lumo_dft': 4.2863655744910245}\n",
      "2024-01-15 18:58:40 [171 | REPORT]: [682|YamboWorkflow|report_wf]: workflow completed successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!verdi process report {run.pk}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d8dd4",
   "metadata": {},
   "source": [
    "Inspecting the report of the process, you can see that indeed the workflow splits the quasiparticle sets and perform a final merge, via the `merge_QP` calcfunction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a3f57",
   "metadata": {},
   "source": [
    "#### How to access the merge QP file and any other file retrieved from a run.\n",
    "\n",
    "The merged database is stored in the AiiDA repository, in principle not able to be accessed \"by hands\". \n",
    "However, there is a trick which consists in the creation of a temporary directory where we copy the file.\n",
    "At that point, we can move it wherever we want, so that we can also use it outside AiiDA (maybe the only \n",
    "reason why we use AiiDA is to easily compute 1000 quasiparticle corrections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb516c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_scheduler-stderr.txt\n",
      "_scheduler-stdout.txt\n",
      "l-aiida.out_HF_and_locXC_gw0_rim_cut_ppa_CPU_1\n",
      "l_p2y_CPU_1\n",
      "l_setup_CPU_1\n",
      "ndb.HF_and_locXC\n",
      "ndb.QP\n",
      "ns.db1\n",
      "o-aiida.out.qp\n",
      "r-aiida.out_HF_and_locXC_gw0_rim_cut_ppa\n",
      "r_setup\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "#a given simulation retrieved folder (you can select the wanted YamboCalculation instead of the run node).\n",
    "retrieved_node = run.outputs.retrieved\n",
    "\n",
    "# Create temporary directory\n",
    "with tempfile.TemporaryDirectory() as dirpath:\n",
    "    # Open the output file from the AiiDA storage and copy content to the temporary file\n",
    "    for filename in retrieved_node.base.repository.list_object_names():\n",
    "        # Create the file with the desired name\n",
    "        temp_file = pathlib.Path(dirpath) / filename\n",
    "        with retrieved_node.open(filename, 'rb') as handle:\n",
    "            temp_file.write_bytes(handle.read())\n",
    "            \n",
    "        print(filename)\n",
    "        \n",
    "        #here you can do the copy of the file:\n",
    "        # os.system(\"cp <dirpath/filename> <your wanted destination>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9833fbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndb.QP_fixed\n"
     ]
    }
   ],
   "source": [
    "#the merged QP\n",
    "retrieved_node = run.outputs.merged_QP\n",
    "\n",
    "# Create temporary directory\n",
    "with tempfile.TemporaryDirectory() as dirpath:\n",
    "    # Open the output file from the AiiDA storage and copy content to the temporary file\n",
    "    for filename in retrieved_node.base.repository.list_object_names():\n",
    "        # Create the file with the desired name\n",
    "        temp_file = pathlib.Path(dirpath) / filename\n",
    "        with retrieved_node.open(filename, 'rb') as handle:\n",
    "            temp_file.write_bytes(handle.read())\n",
    "            \n",
    "        print(filename)\n",
    "        \n",
    "        #here you can do the copy of the file:\n",
    "        # os.system(\"cp <dirpath/filename> <your wanted destination>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df44b0",
   "metadata": {},
   "source": [
    "Why the merged ndb.QP is named `ndb.QP_fixed`? The reason is that there is a sanitizing procedure of\n",
    "the original merged database: as the number of QP is very high, it may happen that some of them is lost or \n",
    "give NaN result. The logic is to find these quasiparticle corrections and replace them with scissor&stretching\n",
    "correction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
