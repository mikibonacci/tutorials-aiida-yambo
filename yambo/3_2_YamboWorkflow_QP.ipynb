{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c65c1d1",
   "metadata": {},
   "source": [
    "# YamboWorkflow to easily compute several quasiparticle corrections\n",
    "\n",
    "Often, several quasiparticle corrections (>100) needs to be computed: we may want to computed interpolated G0W0 bands (with `yambopy` or `wannier90`, for example), or we need it to solve the Bethe-Salpeter equation on top of G0W0 results, which represent the state-of-the-art protocol to compute optical properties of materials.\n",
    "\n",
    "However, this task is a really time consuming: we need to split into several simulation, and the merge back the `ndb.QP` databases containing the quasiparticle corrections. \n",
    "Routinely, this can be done via simple scripts, for a small number of quasiparticle needed, within the `yambopy` package.\n",
    "\n",
    "When we are dealing with demanding simulations, we can exploit the power of AiiDA to automatically obtain the merged quasiparticle database.  \n",
    "\n",
    "The logic is simple: divide et impera. The workflow decides how to distribute the quasiparticle corrections among the calculations, following input parameters provided by the user. \n",
    "This represents the main difference with the standard `YamboWorklow` run. \n",
    "Then, under the hood, the plugin call `yambopy` to perform the final merging.\n",
    "\n",
    "This tutorial will proceed as the previous one, expect for the fact that, before the submission, we will provide the information needed to compute the wanted quasiparticles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb14967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the main inputs:\n",
      "BndsRnXp = 150\n",
      "GbndRnge = 150\n",
      "NGsBlkXp = 4 Ry\n",
      "FFTGvecs = 24 Ry\n",
      "\n",
      "\n",
      "kpoint mesh for nscf: [6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "from aiida import orm, load_profile\n",
    "load_profile()\n",
    "\n",
    "from aiida.plugins import WorkflowFactory\n",
    "from aiida.orm import QueryBuilder\n",
    "from aiida.engine import submit\n",
    "\n",
    "from aiida_quantumespresso.common.types import ElectronicType\n",
    "\n",
    "import yaml\n",
    "\n",
    "qb = QueryBuilder()\n",
    "qb.append(orm.Group, filters={'label': 'Silicon/bulk'}, tag='group')\n",
    "qb.append(orm.StructureData, with_group='group')\n",
    "\n",
    "loaded_structure_id = qb.all()[0][0].pk\n",
    "\n",
    "# Read YAML file\n",
    "with open(\"../configuration/codes_localhost.yaml\", 'r') as stream:\n",
    "    codes = yaml.safe_load(stream)\n",
    "    \n",
    "with open(\"../configuration/resources_localhost.yaml\", 'r') as stream:\n",
    "    resources = yaml.safe_load(stream)\n",
    "    \n",
    "options = {\n",
    "    'pseudo_family':\"PseudoDojo/0.4/PBE/SR/standard/upf\",\n",
    "    'protocol':'fast',\n",
    "    #'parent_id':274, #not necessary to set; if you want it, take ytheour previously nscf id (pk) to skip the DFT part.\n",
    "    'structure_id':loaded_structure_id,\n",
    "}\n",
    "\n",
    "YamboWorkflow = WorkflowFactory('yambo.yambo.yambowf')\n",
    "\n",
    "builder = YamboWorkflow.get_builder_from_protocol(\n",
    "            pw_code = codes['pwcode_id'],\n",
    "            preprocessing_code = codes['yamboprecode_id'],\n",
    "            code = codes['yambocode_id'],\n",
    "            protocol=options['protocol'],\n",
    "            protocol_qe=options['protocol'],\n",
    "            structure= orm.load_node(options['structure_id']),\n",
    "            overrides={\n",
    "                'yres': {\"yambo\": {\n",
    "                    \"parameters\": {\n",
    "                        \"variables\": {\n",
    "                            \"NGsBlkXp\": [4, \"Ry\"],\n",
    "                            \"FFTGvecs\": [24, \"Ry\"],\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            pseudo_family= options['pseudo_family'],\n",
    "            #parent_folder=orm.load_node(options['parent_id']).outputs.remote_folder,\n",
    "            electronic_type=ElectronicType.INSULATOR, #default is METAL: in that case, smearing is used\n",
    "            calc_type='gw', #or 'bse'; default is 'gw'\n",
    ")\n",
    "\n",
    "builder.scf.pw.metadata.options = resources\n",
    "\n",
    "builder.nscf.pw.metadata.options = builder.scf.pw.metadata.options\n",
    "builder.yres.yambo.metadata.options = builder.scf.pw.metadata.options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96b057",
   "metadata": {},
   "source": [
    "### Requesting the YamboWorkflow to compute a specific set of quasiparticles\n",
    "\n",
    "The idea is to split the QP calculation in several subsets, then merge it in a final database. So, at the end of the calculations, the ndb.QP databases are merged in only one database and exposed as an AiiDA SingleFileData type\n",
    "output. \n",
    "The merging procedure is performed using *yambopy*. \n",
    "There are a lot of possibilities to run QP calculations, to be provided in the `QP_subset_dict` input of the YamboWorkflow:\n",
    "\n",
    "(1) provide subset of already wanted QP, already in subsets (i.e. already splitted);\n",
    "\n",
    "```python\n",
    "QP_subset_dict= {\n",
    "    'subsets':[\n",
    "        [[1,1,8,9],[2,2,8,9]], #first subset\n",
    "        [[3,3,8,9],[4,4,8,9]], #second subset\n",
    "                ],\n",
    "}\n",
    "```\n",
    "\n",
    "(2) provide explicit QP, i.e. a list of single QP to be splitted;\n",
    "\n",
    "```python\n",
    "QP_subset_dict= {\n",
    "    'explicit':[\n",
    "        [1,1,8,9],[2,2,8,9],[3,3,8,9],[4,4,8,9], #to be splitted\n",
    "                ],\n",
    "}\n",
    "```\n",
    "\n",
    "(3) provide boundaries for the bands to be computed: [k_i,k_f,b_i,b_f];\n",
    "\n",
    "```python\n",
    "QP_subset_dict= {\n",
    "    'boundaries':{\n",
    "        'k_i':1,    #default=1\n",
    "        'k_f':20,   #default=NK_ibz\n",
    "        'b_i':8,\n",
    "        'b_f':9,\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "(4) provide a range of (DFT) energies where to consider the bands and the k-points to be computed, useful if we don't know the system;\n",
    "    of we want BSE for given energies -- usually, BSE spectra is well converged for 75% of this range. These are generated as \n",
    "    explicit QP, then splitted.\n",
    "    It is possible to provide also: 'range_spectrum', which find the bands to be included in the BSE calculation, including the other bands \n",
    "    outside the range_QP window as scissored -- automatically by yambo in the BSE calc. So the final QP will have \n",
    "    rangeQP bands, but the BSE calc will have all the range_spectrum bands.\n",
    "    These ranges are windows of 2*range, centered at the Fermi level. \n",
    "    If you set the key 'full_bands'=True, all the kpoints are included for each bands. otherwise, only the qp in the window.\n",
    "\n",
    "```python\n",
    "QP_subset_dict= {\n",
    "    'range_QP':3, #eV         , default=nscf_gap_eV*1.2\n",
    "    'range_spectrum':10, #eV\n",
    "\n",
    "}\n",
    "```\n",
    "#### Additional options for (2) and (4)\n",
    "for (2) and (4) there are additional options:\n",
    "    - (a) 'split_bands': split also in bands, not only kpoints the subset. default is True.\n",
    "    - (b) 'extend_QP': it allows to extend the qp after the merging, including QP not explicitely computed\n",
    "        as FD+scissored corrections (see paper HT M Bonacci et al. 2023). Useful in G0W0 interpolations\n",
    "        e.g. within the aiida-yambo-wannier90 plugin.\n",
    "        (b.1) 'consider_only': bands to be only considered explcitely, so the other ones are deleted from the explicit subsets;\n",
    "        (b.2) 'T_smearing': the fake smearing temperature of the correction.\n",
    "\n",
    "```python\n",
    "QP_subset_dict.update({\n",
    "    'split_bands':True, #default\n",
    "    'extend_QP': True, #default is False\n",
    "    'consider_only':[8,9],\n",
    "    'T_smearing':1e-2, #default\n",
    "})\n",
    "```\n",
    "\n",
    "#### Basic usage\n",
    "Usually, the setting that we should provide are: \n",
    "\n",
    "(a) 'qp_per_subset':20; #how many qp in each splitted subset.\n",
    "(b) 'parallel_runs':4; to be submitted at the same time remotely. then the remote is deleted, as the qp is stored locally,\n",
    "(c) 'resources':para_QP, #see below\n",
    "(d) 'parallelism':res_QP, #see below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf66e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_QP = {}\n",
    "para_QP['SE_CPU'] = '2 2 4'\n",
    "para_QP['SE_ROLEs'] = 'q qp b'\n",
    "res_QP = {\n",
    "                        'num_machines': 1,\n",
    "                        'num_mpiprocs_per_machine': 1,\n",
    "                        'num_cores_per_mpiproc': 1,\n",
    "            }\n",
    "\n",
    "\n",
    "QP_subset_dict= {\n",
    "    'range_QP':3, #eV         , default=nscf_gap_eV*1.2\n",
    "    'full_bands':True,\n",
    "    'consider_only':[4,5], \n",
    "    'qp_per_subset': 10,\n",
    "    'parallel_runs':4,\n",
    "\n",
    "}\n",
    "\n",
    "QP_subset_dict.update({\n",
    "    'resources':res_QP, #default is the same as previous GW\n",
    "    'parallelism': para_QP, #default is the same as previous GW\n",
    "\n",
    "})\n",
    "\n",
    "\n",
    "builder.QP_subset_dict= orm.Dict(dict=QP_subset_dict) #set this if you want to compute also QP after the single GW calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e482c",
   "metadata": {},
   "source": [
    "### Submission phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d34e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab19daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/.conda/envs/aiida/lib/python3.9/site-packages/aiida_quantumespresso/calculations/pw.py:211: UserWarning: `parent_folder` not provided for `nscf` calculation. For work chains wrapping this calculation, you can disable this warning by excluding the `parent_folder` when exposing the inputs of the `PwCalculation`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uuid: 66eb0731-8fc9-4f05-b42a-6d84a4287dfb (pk: 2699) (aiida.workflows:yambo.yambo.yambowf)\n"
     ]
    }
   ],
   "source": [
    "if run:\n",
    "    print('run is already running -> {}'.format(run.pk))\n",
    "    print('sure that you want to run again?, if so, copy the else instruction in the cell below and run!')\n",
    "else:\n",
    "    run = submit(builder)\n",
    "\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904aae0d",
   "metadata": {},
   "source": [
    "### Inspecting the outputs\n",
    "\n",
    "Suppose that your calculation finished successfully, then you can access the outputs via the output method of the run instance. All the outputs of YamboRestart and YamboCalculation are inherited here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8202754e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.is_finished_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9732396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[22m2024-02-15 14:25:09 [1032 | REPORT]: [2699|YamboWorkflow|start_workflow]: no previous pw calculation found, we will start from scratch\n",
      "2024-02-15 14:25:09 [1033 | REPORT]: [2699|YamboWorkflow|start_workflow]:  workflow initilization step completed.\n",
      "2024-02-15 14:25:09 [1034 | REPORT]: [2699|YamboWorkflow|can_continue]: the workflow continues with a scf calculation\n",
      "2024-02-15 14:25:09 [1035 | REPORT]: [2699|YamboWorkflow|perform_next]: performing a scf calculation\n",
      "2024-02-15 14:25:11 [1036 | REPORT]:   [2702|PwBaseWorkChain|run_process]: launching PwCalculation<2735> iteration #1\n",
      "2024-02-15 14:25:22 [1042 | REPORT]:   [2702|PwBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2024-02-15 14:25:22 [1043 | REPORT]:   [2702|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2024-02-15 14:25:22 [1044 | REPORT]: [2699|YamboWorkflow|can_continue]: the workflow continues with a nscf calculation\n",
      "2024-02-15 14:25:22 [1045 | REPORT]: [2699|YamboWorkflow|perform_next]: performing a nscf calculation\n",
      "2024-02-15 14:25:24 [1046 | REPORT]:   [2748|PwBaseWorkChain|run_process]: launching PwCalculation<2751> iteration #1\n",
      "2024-02-15 14:26:12 [1052 | REPORT]:   [2748|PwBaseWorkChain|results]: work chain completed after 1 iterations\n",
      "2024-02-15 14:26:12 [1053 | REPORT]:   [2748|PwBaseWorkChain|on_terminated]: remote folders will not be cleaned\n",
      "2024-02-15 14:26:13 [1054 | REPORT]: [2699|YamboWorkflow|can_continue]: the workflow continues with a yambo calculation\n",
      "2024-02-15 14:26:13 [1055 | REPORT]: [2699|YamboWorkflow|perform_next]: performing a yambo calculation\n",
      "2024-02-15 14:26:15 [1056 | REPORT]:   [2766|YamboRestart|run_process]: launching YamboCalculation<2767> iteration #1\n",
      "2024-02-15 14:28:05 [1074 | REPORT]:   [2766|YamboRestart|results]: work chain completed after 1 iterations\n",
      "2024-02-15 14:28:05 [1075 | REPORT]:   [2766|YamboRestart|on_terminated]: remote folders will not be cleaned\n",
      "2024-02-15 14:28:06 [1076 | REPORT]: [2699|YamboWorkflow|can_continue]: the workflow continues with a QP splitter calculation\n",
      "2024-02-15 14:28:06 [1077 | REPORT]: [2699|YamboWorkflow|perform_next]: performing a QP splitter calculation\n",
      "2024-02-15 14:28:06 [1078 | REPORT]: [2699|YamboWorkflow|perform_next]: range of energy for QP: 3 eV\n",
      "2024-02-15 14:28:07 [1079 | REPORT]: [2699|YamboWorkflow|perform_next]: subsets: [[[1, 1, 4, 4], [1, 1, 5, 5], [2, 2, 4, 4], [2, 2, 5, 5], [3, 3, 4, 4], [3, 3, 5, 5], [4, 4, 4, 4], [4, 4, 5, 5], [5, 5, 4, 4], [5, 5, 5, 5]], [[6, 6, 4, 4], [6, 6, 5, 5], [7, 7, 4, 4], [7, 7, 5, 5], [8, 8, 4, 4], [8, 8, 5, 5], [9, 9, 4, 4], [9, 9, 5, 5], [10, 10, 4, 4], [10, 10, 5, 5]], [[11, 11, 4, 4], [11, 11, 5, 5], [12, 12, 4, 4], [12, 12, 5, 5], [13, 13, 4, 4], [13, 13, 5, 5], [14, 14, 4, 4], [14, 14, 5, 5], [15, 15, 4, 4], [15, 15, 5, 5]], [[16, 16, 4, 4], [16, 16, 5, 5]]]\n",
      "2024-02-15 14:28:07 [1080 | REPORT]: [2699|YamboWorkflow|perform_next]: launchiing YamboRestart <2798> for QP, iteration#1\n",
      "2024-02-15 14:28:08 [1081 | REPORT]: [2699|YamboWorkflow|perform_next]: launchiing YamboRestart <2800> for QP, iteration#2\n",
      "2024-02-15 14:28:09 [1082 | REPORT]: [2699|YamboWorkflow|perform_next]: launchiing YamboRestart <2802> for QP, iteration#3\n",
      "2024-02-15 14:28:10 [1083 | REPORT]: [2699|YamboWorkflow|perform_next]: launchiing YamboRestart <2804> for QP, iteration#4\n",
      "2024-02-15 14:28:11 [1084 | REPORT]:   [2798|YamboRestart|run_process]: launching YamboCalculation<2805> iteration #1\n",
      "2024-02-15 14:28:12 [1085 | REPORT]:   [2800|YamboRestart|run_process]: launching YamboCalculation<2806> iteration #1\n",
      "2024-02-15 14:28:14 [1086 | REPORT]:   [2802|YamboRestart|run_process]: launching YamboCalculation<2807> iteration #1\n",
      "2024-02-15 14:28:16 [1087 | REPORT]:   [2804|YamboRestart|run_process]: launching YamboCalculation<2808> iteration #1\n",
      "2024-02-15 14:29:19 [1088 | REPORT]:   [2798|YamboRestart|results]: work chain completed after 1 iterations\n",
      "2024-02-15 14:29:19 [1089 | REPORT]:   [2798|YamboRestart|on_terminated]: cleaned remote folders of calculations: 2805\n",
      "2024-02-15 14:32:18 [1099 | REPORT]:   [2800|YamboRestart|results]: work chain completed after 1 iterations\n",
      "2024-02-15 14:32:19 [1100 | REPORT]:   [2800|YamboRestart|on_terminated]: cleaned remote folders of calculations: 2806\n",
      "2024-02-15 14:33:02 [1101 | REPORT]:   [2802|YamboRestart|results]: work chain completed after 1 iterations\n",
      "2024-02-15 14:33:02 [1102 | REPORT]:   [2802|YamboRestart|on_terminated]: cleaned remote folders of calculations: 2807\n",
      "2024-02-15 14:33:10 [1103 | REPORT]:   [2804|YamboRestart|results]: work chain completed after 1 iterations\n",
      "2024-02-15 14:33:10 [1104 | REPORT]:   [2804|YamboRestart|on_terminated]: cleaned remote folders of calculations: 2808\n",
      "2024-02-15 14:33:11 [1105 | REPORT]: [2699|YamboWorkflow|can_continue]: workflow is finished\n",
      "2024-02-15 14:33:11 [1106 | REPORT]: [2699|YamboWorkflow|post_processing_needed]: merge QP needed\n",
      "2024-02-15 14:33:11 [1107 | REPORT]: [2699|YamboWorkflow|run_post_process]: run merge QP\n",
      "2024-02-15 14:33:36 [1108 | REPORT]: [2699|YamboWorkflow|report_wf]: workflow completed successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!verdi process report {run.pk}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d8dd4",
   "metadata": {},
   "source": [
    "Inspecting the report of the process, you can see that indeed the workflow splits the quasiparticle sets and perform a final merge, via the `merge_QP` calcfunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b419f211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SOC': False,\n",
       " 'QP_pk': 2860,\n",
       " 'c_max': 5,\n",
       " 'q_ind': 13,\n",
       " 'v_min': 4,\n",
       " 'gap_GW': 1.0576,\n",
       " 'nscf_pk': 2751,\n",
       " 'GW_k_c_ind': 13,\n",
       " 'GW_k_v_ind': 1,\n",
       " 'candidate_for_BSE': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.outputs.output_ywfl_parameters.get_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cee218d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'soc': False,\n",
       " 'gap_': [[1, 1, 4, 4], [13, 13, 5, 5]],\n",
       " 'homo_k': 1,\n",
       " 'lumo_k': 13,\n",
       " 'valence': 4,\n",
       " 'gap_type': 'indirect',\n",
       " 'conduction': 5,\n",
       " 'nscf_gap_eV': 0.665,\n",
       " 'dft_predicted': 'semiconductor/insulator',\n",
       " 'number_of_kpoints': 16,\n",
       " 'magnetic_calculation': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.outputs.nscf_mapping.get_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a3f57",
   "metadata": {},
   "source": [
    "#### How to access the merge QP file and any other file retrieved from a run.\n",
    "\n",
    "The merged database is stored in the AiiDA repository, in principle not able to be accessed \"by hands\". \n",
    "However, there is a trick which consists in the creation of a temporary directory where we copy the file.\n",
    "At that point, we can move it wherever we want, so that we can also use it outside AiiDA (maybe the only \n",
    "reason why we use AiiDA is to easily compute 1000 quasiparticle corrections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb516c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_scheduler-stderr.txt\n",
      "_scheduler-stdout.txt\n",
      "l-aiida.out_HF_and_locXC_gw0_ppa_el_el_corr\n",
      "l_p2y\n",
      "l_setup\n",
      "ndb.HF_and_locXC\n",
      "ndb.QP\n",
      "ns.db1\n",
      "o-aiida.out.qp\n",
      "r-aiida.out_HF_and_locXC_gw0_ppa_el_el_corr\n",
      "r_setup\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "#a given simulation retrieved folder (you can select the wanted YamboCalculation instead of the run node).\n",
    "retrieved_node = run.outputs.retrieved\n",
    "\n",
    "# Create temporary directory\n",
    "with tempfile.TemporaryDirectory() as dirpath:\n",
    "    # Open the output file from the AiiDA storage and copy content to the temporary file\n",
    "    for filename in retrieved_node.base.repository.list_object_names():\n",
    "        # Create the file with the desired name\n",
    "        temp_file = pathlib.Path(dirpath) / filename\n",
    "        with retrieved_node.open(filename, 'rb') as handle:\n",
    "            temp_file.write_bytes(handle.read())\n",
    "            \n",
    "        print(filename)\n",
    "        \n",
    "        #here you can do the copy of the file:\n",
    "        # os.system(\"cp <dirpath/filename> <your wanted destination>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9833fbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndb.QP_fixed\n"
     ]
    }
   ],
   "source": [
    "#the merged QP\n",
    "retrieved_node = run.outputs.merged_QP\n",
    "\n",
    "# Create temporary directory\n",
    "with tempfile.TemporaryDirectory() as dirpath:\n",
    "    # Open the output file from the AiiDA storage and copy content to the temporary file\n",
    "    for filename in retrieved_node.base.repository.list_object_names():\n",
    "        # Create the file with the desired name\n",
    "        temp_file = pathlib.Path(dirpath) / filename\n",
    "        with retrieved_node.open(filename, 'rb') as handle:\n",
    "            temp_file.write_bytes(handle.read())\n",
    "            \n",
    "        print(filename)\n",
    "        \n",
    "        #here you can do the copy of the file:\n",
    "        # os.system(\"cp <dirpath/filename> <your wanted destination>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df44b0",
   "metadata": {},
   "source": [
    "Why the merged ndb.QP is named `ndb.QP_fixed`? The reason is that there is a sanitizing procedure of\n",
    "the original merged database: as the number of QP is very high, it may happen that some of them is lost or \n",
    "give NaN result. The logic is to find these quasiparticle corrections and replace them with scissor&stretching\n",
    "correction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
